{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FuzzyWuzzy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE4lF56j2Nwz",
        "colab_type": "text"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5AL2ApX2Nw2",
        "colab_type": "text"
      },
      "source": [
        "This notebook implements logistic regression and gradient boosting methods on both basics and fuzzy-wuzzy features of the quora question pairs dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm7TN-u02Nw3",
        "colab_type": "text"
      },
      "source": [
        "### Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJgaAxc2Nw5",
        "colab_type": "code",
        "outputId": "2c2d774b-d638-4c9e-8d46-62f56e25308c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python -V"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPlOpTbHzR11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4a9a9a9-c351-4d6b-8b38-1645d1f9211d"
      },
      "source": [
        "!pip3 install fuzzywuzzy\n",
        "!pip3 install python-Levenshtein\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (46.1.3)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144797 sha256=5dcdc4a7c67e286d81ce4ae150e8fd9a8a3e6314c8ec974aa21cc308e604793c\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.24.3)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp36-none-any.whl size=74031 sha256=03cdd640ffad873902aff0ab12670f7a72363d92ce2a6a4c65f7820585eb7b60\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ef4d09bca6772b23dc69d80099351e56f9ffb1742b694f317312bee68aedc52f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1DoLCgUEzODG",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW8YgvLS3Vvs",
        "colab_type": "code",
        "outputId": "e13dd57e-788e-40de-dca0-c4462e515303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd '/content/drive/My Drive/NLP/'\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/NLP\n",
            "data\t\t   FuzzyWuzzy.ipynb\t\t\t   Untitled0.ipynb\n",
            "Fuzzy_Quora.ipynb  Sentence_Transformers_embbedings.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqHYq6bv2Nxf",
        "colab_type": "text"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0-SZMKt32h_1",
        "outputId": "5ea0d206-644d-43bc-bcbd-a6925935c0ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_path = \"data/\" #change datapath to fit yours\n",
        "\n",
        "df_train = pd.read_csv(data_path+\"train.csv\")\n",
        "\n",
        "df_train = df_train.dropna()\n",
        "df_train.shape[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "404287"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhAaKLFP2Nxo",
        "colab_type": "code",
        "outputId": "9f532401-d458-4a6f-e112-0bc6d807f0d8",
        "colab": {}
      },
      "source": [
        "df_train = df_train.drop(['id', 'qid1', 'qid2'], axis=1)\n",
        "df_train[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Should I buy tiago?</td>\n",
              "      <td>What keeps childern active and far from phone ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>When do you use シ instead of し?</td>\n",
              "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
              "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question1  \\\n",
              "0  What is the step by step guide to invest in sh...   \n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2  How can I increase the speed of my internet co...   \n",
              "3  Why am I mentally very lonely? How can I solve...   \n",
              "4  Which one dissolve in water quikly sugar, salt...   \n",
              "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
              "6                                Should I buy tiago?   \n",
              "7                     How can I be a good geologist?   \n",
              "8                    When do you use シ instead of し?   \n",
              "9  Motorola (company): Can I hack my Charter Moto...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  \n",
              "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
              "6  What keeps childern active and far from phone ...             0  \n",
              "7          What should I do to be a great geologist?             1  \n",
              "8              When do you use \"&\" instead of \"and\"?             0  \n",
              "9  How do I hack Motorola DCX3400 for free internet?             0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IPtU8h4F3M0K",
        "colab": {}
      },
      "source": [
        "df_train[\"question1\"] = df_train[\"question1\"].astype(\"str\")\n",
        "df_train[\"question2\"] = df_train[\"question2\"].astype('str')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "act61as62Nx7",
        "colab_type": "text"
      },
      "source": [
        "## Features 1 : basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWgF1g5R2Nx9",
        "colab_type": "text"
      },
      "source": [
        "We first compute basic features from dataset's sentences (lengths, #common words,...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_pNe4ne2Nx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of questions\n",
        "df_train['len_q1'] = df_train.question1.apply(lambda x: len(str(x)))\n",
        "df_train['len_q2'] = df_train.question2.apply(lambda x: len(str(x)))\n",
        "# Difference between length of the two questions\n",
        "df_train['diff_len'] = df_train.len_q1 - df_train.len_q2\n",
        "# Character length without spaces\n",
        "df_train['len_char_q1'] = df_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "df_train['len_char_q2'] = df_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "# Number of words\n",
        "df_train['len_word_q1'] = df_train.question1.apply(lambda x: len(str(x).split()))\n",
        "df_train['len_word_q2'] = df_train.question2.apply(lambda x: len(str(x).split()))\n",
        "# Number of common words in the two questions\n",
        "df_train['common_words'] = df_train.apply(lambda x: len(set(str(x['question1'])\n",
        "    .lower().split())\n",
        "    .intersection(set(str(x['question2'])\n",
        "    .lower().split()))), axis=1)\n",
        "\n",
        "basics = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2', 'common_words']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvNay9hZ2NyE",
        "colab_type": "text"
      },
      "source": [
        "## Features 2 : fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCAKvQim2NyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['fuzz_qratio'] = df_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "df_train['fuzz_WRatio'] = df_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "df_train['fuzz_partial_ratio'] = df_train.apply(lambda x: fuzz.partial_ratio(\n",
        "    str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "df_train['fuzz_partial_token_set_ratio'] = df_train.apply(lambda x: fuzz.partial_token_set_ratio(\n",
        "    str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "df_train['fuzz_partial_token_sort_ratio'] = df_train.apply(lambda x: fuzz.partial_token_sort_ratio(\n",
        "    str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "df_train['fuzz_token_set_ratio'] = df_train.apply(lambda x: fuzz.token_set_ratio(\n",
        "str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "df_train['fuzz_token_sort_ratio'] = df_train.apply(lambda x: fuzz.token_sort_ratio(\n",
        "    str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "fuzzys = ['fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
        "          'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio', 'fuzz_token_sort_ratio']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_QT4brA2NyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_train.to_pickle('data/df_train_wfs1fs2.plk')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtWHwisp2Nyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_pickle('data/df_train_wfs1fs2.plk')\n",
        "fuzzys = ['fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
        "          'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio', 'fuzz_token_sort_ratio']\n",
        "basics = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2', 'common_words']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsdGco3v2Nyo",
        "colab_type": "text"
      },
      "source": [
        "# Preparing datasets for learning methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_NMkzI42Nyp",
        "colab_type": "text"
      },
      "source": [
        "We now train and evaluate **logistic regression** and **gradient boosting** on those features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YQU90Tn42Nyv",
        "colab_type": "code",
        "outputId": "cd11af5d-432e-4615-dfc9-17c924301c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "df_train[0:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "      <th>len_q1</th>\n",
              "      <th>len_q2</th>\n",
              "      <th>diff_len</th>\n",
              "      <th>len_char_q1</th>\n",
              "      <th>len_char_q2</th>\n",
              "      <th>len_word_q1</th>\n",
              "      <th>len_word_q2</th>\n",
              "      <th>common_words</th>\n",
              "      <th>fuzz_qratio</th>\n",
              "      <th>fuzz_WRatio</th>\n",
              "      <th>fuzz_partial_ratio</th>\n",
              "      <th>fuzz_partial_token_set_ratio</th>\n",
              "      <th>fuzz_partial_token_sort_ratio</th>\n",
              "      <th>fuzz_token_set_ratio</th>\n",
              "      <th>fuzz_token_sort_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>57</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>93</td>\n",
              "      <td>95</td>\n",
              "      <td>98</td>\n",
              "      <td>100</td>\n",
              "      <td>89</td>\n",
              "      <td>100</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>88</td>\n",
              "      <td>-37</td>\n",
              "      <td>21</td>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>66</td>\n",
              "      <td>86</td>\n",
              "      <td>73</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>86</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "      <td>73</td>\n",
              "      <td>59</td>\n",
              "      <td>14</td>\n",
              "      <td>25</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>63</td>\n",
              "      <td>53</td>\n",
              "      <td>100</td>\n",
              "      <td>71</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>65</td>\n",
              "      <td>-15</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>25</td>\n",
              "      <td>18</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>46</td>\n",
              "      <td>86</td>\n",
              "      <td>54</td>\n",
              "      <td>100</td>\n",
              "      <td>63</td>\n",
              "      <td>67</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question1  ... fuzz_token_sort_ratio\n",
              "0  What is the step by step guide to invest in sh...  ...                    93\n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...  ...                    63\n",
              "2  How can I increase the speed of my internet co...  ...                    66\n",
              "3  Why am I mentally very lonely? How can I solve...  ...                    36\n",
              "4  Which one dissolve in water quikly sugar, salt...  ...                    47\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTn1OAqG2Ny0",
        "colab_type": "text"
      },
      "source": [
        "Since logistic regression is sensitive to the scale of the features we will start by standardizing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz_KVFFu2Ny0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUc09EOO2Ny5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_train.is_duplicate.values\n",
        "y = y.astype('float32').reshape(-1, 1)\n",
        "X = df_train[basics+fuzzys]\n",
        "X = X.replace([np.inf, -np.inf], np.nan).fillna(0).values\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G4uZ6072Ny-",
        "colab_type": "text"
      },
      "source": [
        " Then we separate data in 80(training)-20(validation) sets for validation purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4JHFve82Ny_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "#shuffling data to avoid bias\n",
        "total = y.shape[0]\n",
        "index = np.arange(total) \n",
        "np.random.shuffle(index)\n",
        "\n",
        "#we split the dataset into 20% val - 80% train\n",
        "split = 2*total // 10 \n",
        "index_val = index[:split]\n",
        "index_train = index[split:]\n",
        "\n",
        "x_train = X[index_train]\n",
        "y_train = np.ravel(y[index_train])\n",
        "x_val = X[index_val]\n",
        "y_val = np.ravel(y[index_val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgLvzzbf2NzE",
        "colab_type": "code",
        "outputId": "982ef748-c067-480c-e9af-64475fc23fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"x_train shape is: {}\".format(x_train.shape) + \" and x_val shape is: {}\".format(x_val.shape))\n",
        "print(\"y_train shape is: {}\".format(y_train.shape) + \" and y_val shape is: {}\".format(y_val.shape))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape is: (323430, 15) and x_val shape is: (80857, 15)\n",
            "y_train shape is: (323430,) and y_val shape is: (80857,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UixYTEU2NzI",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlAgSpkq2NzJ",
        "colab_type": "code",
        "outputId": "8e499c38-6c9b-4adb-90e4-dcd8d4ab42e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "#we run a logistic regression with SGD classifier on training data\n",
        "lreg = linear_model.SGDClassifier(loss='log',verbose=1)\n",
        "lreg.fit(x_train, y_train)\n",
        "\n",
        "#then compute predictions on validation set\n",
        "lreg_preds_prob = lreg.predict_proba(x_val)\n",
        "lreg_preds = lreg.predict(x_val)\n",
        "#and on training set\n",
        "lreg_preds_prob_t = lreg.predict_proba(x_train)\n",
        "lreg_preds_t = lreg.predict(x_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 1.94, NNZs: 15, Bias: -0.923796, T: 323430, Avg. loss: 0.810572\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.72, NNZs: 15, Bias: -1.078893, T: 646860, Avg. loss: 0.567442\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.78, NNZs: 15, Bias: -0.819547, T: 970290, Avg. loss: 0.562302\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.74, NNZs: 15, Bias: -0.891265, T: 1293720, Avg. loss: 0.560250\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.69, NNZs: 15, Bias: -0.893622, T: 1617150, Avg. loss: 0.559237\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.72, NNZs: 15, Bias: -0.988568, T: 1940580, Avg. loss: 0.558502\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.68, NNZs: 15, Bias: -0.878439, T: 2264010, Avg. loss: 0.557944\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.73, NNZs: 15, Bias: -0.951027, T: 2587440, Avg. loss: 0.557534\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.74, NNZs: 15, Bias: -0.952403, T: 2910870, Avg. loss: 0.557183\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.74, NNZs: 15, Bias: -0.894677, T: 3234300, Avg. loss: 0.557013\n",
            "Total training time: 0.76 seconds.\n",
            "Convergence after 10 epochs took 0.76 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imU5OEHXQ9qm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3a3c23b5-4bd3-49df-d10b-7febb140923f"
      },
      "source": [
        "###### ###### ######      RESULTS on training set   ###### ###### ###### \n",
        "\n",
        "lreg_accuracy = metrics.accuracy_score(y_train,lreg_preds_t)\n",
        "lreg_f1 = metrics.f1_score(y_train,lreg_preds_t)\n",
        "lreg_loss = metrics.log_loss(y_train,lreg_preds_prob_t)\n",
        "\n",
        "print(\"Logistic regression accuracy on training set: %0.3f\" % lreg_accuracy)\n",
        "print(\"Logistic regression F1score on training set: %0.3f\" % lreg_f1)\n",
        "print(\"Logistic regression loss on training set: %0.3f\" % lreg_loss)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression accuracy on training set: 0.662\n",
            "Logistic regression F1score on training set: 0.520\n",
            "Logistic regression loss on training set: 0.556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fH7GFIWxvvk",
        "colab_type": "code",
        "outputId": "c1e82539-3e13-45ea-bc53-2487f1df9459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "###### ###### ######      RESULTS on validation set    ###### ###### ###### \n",
        "\n",
        "lreg_accuracy = metrics.accuracy_score(y_val,lreg_preds)\n",
        "lreg_f1 = metrics.f1_score(y_val,lreg_preds)\n",
        "lreg_loss = metrics.log_loss(y_val,lreg_preds_prob)\n",
        "\n",
        "print(\"Logistic regression accuracy on validation set: %0.3f\" % lreg_accuracy)\n",
        "print(\"Logistic regression F1score on validation set: %0.3f\" % lreg_f1)\n",
        "print(\"Logistic regression loss on validation set: %0.3f\" % lreg_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression accuracy on validation set: 0.663\n",
            "Logistic regression F1score on validation set: 0.501\n",
            "Logistic regression loss on validation set: 0.555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79u4_t_2NzP",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4P6aLQo2NzQ",
        "colab_type": "code",
        "outputId": "fe62c1b1-3a98-4273-aee2-19805b040ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "#we run a gradient boosting binary classification on training data\n",
        "params = dict()\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['loss'] = ['logloss']\n",
        "params['learning_rate'] = 0.02\n",
        "params['max_depth'] = 4\n",
        "params['eval_metric'] = ['logloss', 'error']\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_val, label=y_val)\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "\n",
        "boosting = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=20, verbose_eval=100)\n",
        "xgboost_preds = (boosting.predict(d_valid) >= 0.5).astype(int)\n",
        "xgboost_preds_proba = (boosting.predict(d_valid)).astype(float)\n",
        "xgboost_preds_t = (boosting.predict(d_train) >= 0.5).astype(int)\n",
        "xgboost_preds_proba_t = (boosting.predict(d_train)).astype(float)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.687866\ttrain-error:0.305504\tvalid-logloss:0.687854\tvalid-error:0.304612\n",
            "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
            "\n",
            "Will train until valid-error hasn't improved in 20 rounds.\n",
            "[100]\ttrain-logloss:0.526235\ttrain-error:0.296398\tvalid-logloss:0.526048\tvalid-error:0.293902\n",
            "[200]\ttrain-logloss:0.507651\ttrain-error:0.291896\tvalid-logloss:0.507794\tvalid-error:0.289746\n",
            "[300]\ttrain-logloss:0.501285\ttrain-error:0.288072\tvalid-logloss:0.501693\tvalid-error:0.286827\n",
            "[400]\ttrain-logloss:0.496418\ttrain-error:0.284952\tvalid-logloss:0.49717\tvalid-error:0.283884\n",
            "[500]\ttrain-logloss:0.492666\ttrain-error:0.281424\tvalid-logloss:0.493887\tvalid-error:0.280928\n",
            "[600]\ttrain-logloss:0.489615\ttrain-error:0.279421\tvalid-logloss:0.491166\tvalid-error:0.278578\n",
            "Stopping. Best iteration:\n",
            "[600]\ttrain-logloss:0.489615\ttrain-error:0.279421\tvalid-logloss:0.491166\tvalid-error:0.278578\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97uA-0cwSjVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9e93edc9-e848-4378-e865-241b7dddd35f"
      },
      "source": [
        "###### ###### ######      RESULTS     ###### ###### ###### \n",
        "\n",
        "xgboost_accuracy = metrics.accuracy_score(y_train,xgboost_preds_t)\n",
        "xgboost_f1 = metrics.f1_score(y_train,xgboost_preds_t)\n",
        "xgboost_loss = metrics.log_loss(y_train,xgboost_preds_proba_t)\n",
        "print(\"Gradient boosting accuracy on training set: %0.3f\" % xgboost_accuracy)\n",
        "print(\"Gradient boosting F1score on training set: %0.3f\" % xgboost_f1)\n",
        "print(\"Gradient boosting loss on training set: %0.3f\" % xgboost_loss)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient boosting accuracy on training set: 0.721\n",
            "Gradient boosting F1score on training set: 0.658\n",
            "Gradient boosting loss on training set: 0.489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLJWWNKQGvAT",
        "colab_type": "code",
        "outputId": "bf33fc75-4075-4a53-8858-998aff201c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "###### ###### ######      RESULTS     ###### ###### ###### \n",
        "\n",
        "xgboost_accuracy = metrics.accuracy_score(y_val,xgboost_preds)\n",
        "xgboost_f1 = metrics.f1_score(y_val,xgboost_preds)\n",
        "xgboost_loss = metrics.log_loss(y_val,xgboost_preds_proba)\n",
        "print(\"Gradient boosting accuracy on validation set: %0.3f\" % xgboost_accuracy)\n",
        "print(\"Gradient boosting F1score on validation set: %0.3f\" % xgboost_f1)\n",
        "print(\"Gradient boosting loss on validation set: %0.3f\" % xgboost_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient boosting accuracy on validation set: 0.721\n",
            "Gradient boosting F1score on validation set: 0.657\n",
            "Gradient boosting loss on validation set: 0.491\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}