{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaLSTM_model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP_uhltjglgu",
        "colab_type": "code",
        "outputId": "a1e065ac-8d9c-480c-a30d-0ee7e48a5f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "# Mount drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "%cd /content\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd 'drive/My Drive/Colab Notebooks/NLP/project/code'\n",
        "%ls -l\n",
        "print(os.listdir())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/NLP/project/code\n",
            "total 3546\n",
            "-rw------- 1 root root 109039 May 10 14:28 Benchmark_embeddings_advanced.ipynb\n",
            "-rw------- 1 root root 318699 May 14 16:26 Benchmark_embeddings.ipynb\n",
            "-rw------- 1 root root   3534 Apr 22 16:32 cleaning.py\n",
            "-rw------- 1 root root  60575 May 14 19:18 combine_features.ipynb\n",
            "-rw------- 1 root root   4341 May 13 18:02 compute_embeddings.py\n",
            "-rw------- 1 root root  62104 May 14 19:26 MaLSTM_model.ipynb\n",
            "-rw------- 1 root root 363996 May  9 10:48 master.zip\n",
            "-rw------- 1 root root 363996 May  9 10:49 master.zip.1\n",
            "-rw------- 1 root root 363996 May  9 10:51 master.zip.2\n",
            "-rw------- 1 root root   4528 May 14 12:49 modeling.py\n",
            "drwx------ 2 root root   4096 May  3 09:43 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw------- 1 root root  21800 Apr 25 09:06 pytorch_tuto.ipynb\n",
            "drwx------ 2 root root   4096 May  1 16:25 \u001b[01;34msent2vec-master\u001b[0m/\n",
            "-rw------- 1 root root 926824 May 10 10:38 test.h5\n",
            "-rw------- 1 root root   4912 May 13 18:58 transform_dataset.py\n",
            "-rw------- 1 root root 808814 May  4 08:33 Untitled0.ipynb\n",
            "-rw------- 1 root root  22808 Apr 22 16:46 Untitled.ipynb\n",
            "-rw------- 1 root root   3545 Apr 22 16:32 utils_cleaning.py\n",
            "-rw------- 1 root root  21429 May  7 07:34 Vincent_doc2vec.ipynb\n",
            "-rw------- 1 root root  20152 Apr 25 16:47 Vincent.ipynb\n",
            "-rw------- 1 root root  83265 May  3 09:32 Vincent_SBERT.ipynb\n",
            "-rw------- 1 root root  50693 May  4 13:15 Vincent_tfidf.ipynb\n",
            "['cleaning.py', 'utils_cleaning.py', 'Untitled.ipynb', '.ipynb_checkpoints', 'pytorch_tuto.ipynb', '.vector_cache', 'Vincent.ipynb', 'sent2vec-master', 'Vincent_SBERT.ipynb', '__pycache__', 'Untitled0.ipynb', 'Vincent_tfidf.ipynb', 'Vincent_doc2vec.ipynb', 'master.zip', 'master.zip.1', 'master.zip.2', 'test.h5', 'Benchmark_embeddings_advanced.ipynb', 'compute_embeddings.py', 'transform_dataset.py', 'modeling.py', 'Benchmark_embeddings.ipynb', 'combine_features.ipynb', 'MaLSTM_model.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kZs1qkrgv4i",
        "colab_type": "code",
        "outputId": "17040f0f-159f-4418-cc88-fc7bd388fc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from string import punctuation\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import scipy.sparse as sp\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "# from gensim.test.utils import datapath, get_tmpfile\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data import Field, LabelField, TabularDataset, Iterator, BucketIterator\n",
        "from torchtext import vocab\n",
        "import torch.optim as optim\n",
        "\n",
        "import transform_dataset\n",
        "import compute_embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc1kTyPFqj2n",
        "colab_type": "text"
      },
      "source": [
        "### Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJhw75c3gv_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for google news, need to download and save in an appropriate format\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "wv.save_word2vec_format('../embeddings/word2vec-google-news-300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAdr_6ln_qd",
        "colab_type": "code",
        "outputId": "2d140bba-9db4-4192-fd12-647f70fb10ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from torchtext.vocab import Vectors\n",
        "vectors = Vectors(name='word2vec-google-news-300', cache='../embeddings')\n",
        "\n",
        "# Glove and FastText are easier to manage (download on the fly in the right format)\n",
        "# vectors = vocab.GloVe(name='600B', dim=300)\n",
        "# vectors = vocab.FastText(language='en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3000000 [00:00<?, ?it/s]Skipping token b'3000000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 2999941/3000000 [15:21<00:00, 1225.00it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPtW6cWak6Rp",
        "colab_type": "code",
        "outputId": "f20e6e1e-62cd-43e1-e3ae-27d161be9bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# alternative with wiki simple (small embedding)\n",
        "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec -P ../embeddings/\n",
        "# TEXT.vocab.load_vectors('../embeddings/wiki.simple.vec')\n",
        "# vectors = Vectors('../embeddings/wiki.simple.vec')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-13 10:28:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 293187541 (280M) [binary/octet-stream]\n",
            "Saving to: ‘../embeddings/wiki.simple.vec’\n",
            "\n",
            "wiki.simple.vec     100%[===================>] 279.60M  11.9MB/s    in 25s     \n",
            "\n",
            "2020-05-13 10:29:25 (11.3 MB/s) - ‘../embeddings/wiki.simple.vec’ saved [293187541/293187541]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrO7qs1dqmeM",
        "colab_type": "text"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV8HvpY_kvAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"../data/train.csv\"\n",
        "tokenizer = nltk.word_tokenize\n",
        "# tokenize = lambda x: x.split()\n",
        "# tokenizer = text_to_wordlist\n",
        "\n",
        "data = pd.read_csv(path)\n",
        "questions1 = data['question1'].astype('str').tolist()\n",
        "questions2 = data['question2'].astype('str').tolist()\n",
        "is_duplicates = data['is_duplicate'].tolist()\n",
        "\n",
        "TEXT = Field(\n",
        "        sequential=True,\n",
        "        tokenize = tokenizer,\n",
        "        pad_first = True,\n",
        "        dtype = torch.long,\n",
        "        fix_length = 20,\n",
        "        lower = True,\n",
        "        batch_first = True,\n",
        "        )\n",
        "TARGET = LabelField(use_vocab = False, dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YYTP3Uqmazw",
        "colab_type": "code",
        "outputId": "6d730a18-161d-40dd-cab1-f6b8c39b39a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset = TabularDataset(path, 'csv', [('id', None),\n",
        "                                       ('qid1', None),\n",
        "                                       ('qid2', None),\n",
        "                                       ('question1', TEXT),\n",
        "                                       ('question2', TEXT),\n",
        "                                       ('is_duplicate', TARGET)],\n",
        "                         skip_header=True)\n",
        "\n",
        "train_data, valid_data = dataset.split(\n",
        "                  split_ratio=0.9,\n",
        "                  random_state=random.seed(42)\n",
        "                  )\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "\n",
        "# TEXT.build_vocab(train_data, vectors = vectors)\n",
        "TEXT.build_vocab(train_data)\n",
        "TARGET.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 363861\n",
            "Number of validation examples: 40429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNE3msM4e0Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An2aStAApVQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iter, valid_iter = BucketIterator.splits(\n",
        "        (train_data, valid_data), # we pass in the datasets we want the iterator to draw data from\n",
        "        batch_size=64,\n",
        "        device=device, # if you want to use the GPU, specify the GPU number here\n",
        "        sort_key=lambda x: len(x.question1)+len(x.question2), # the BucketIterator needs to be told what function it should use to group the data.\n",
        "        sort_within_batch=False\n",
        "        # repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX6lwMJFqs2S",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An3v4HD5gwC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class siameseNet(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, vectors = TEXT.vocab.vectors):\n",
        "    super().__init__()\n",
        "    # self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, mode='mean')\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "    self.pdist = nn.PairwiseDistance(p=1.0)\n",
        "\n",
        "  def forward_one(self, x):\n",
        "    output = self.embedding(x)\n",
        "    output = self.lstm(output)\n",
        "    return output\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "    emb1 = self.embedding(input1)\n",
        "    emb2 = self.embedding(input2)\n",
        "    output1, _ = self.lstm(emb1)\n",
        "    output2, _ = self.lstm(emb2)\n",
        "    output = torch.exp(-self.pdist(output1[:,-1,:], output2[:,-1,:]))\n",
        "    # output = nn.CosineSimilarity()(output1[:,-1,:], output2[:,-1,:])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdlGneh5gwFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_shape = TEXT.vocab.vectors.shape\n",
        "INPUT_DIM = emb_shape[0]\n",
        "EMBEDDING_DIM = emb_shape[1]\n",
        "HIDDEN_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = siameseNet(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = torch.nn.MSELoss()\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI7TJWQ3rIrD",
        "colab_type": "text"
      },
      "source": [
        "### Learning and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ1pYpkRgwIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(predictions, labels, thresh=0.5):\n",
        "    predicted_labels = predictions >= thresh\n",
        "    accuracy = (predicted_labels == labels).sum()/len(predictions)\n",
        "    return accuracy.item()\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, thresh=0.5):\n",
        "    # Track the loss\n",
        "    epoch_loss = 0\n",
        "    epoch_TP_FN = 0\n",
        "    epoch_FP_TN = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.question1, batch.question2)\n",
        "        loss = criterion(predictions, batch.is_duplicate)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted_labels = predictions >= thresh\n",
        "        epoch_TP_FN += (predicted_labels==batch.is_duplicate).sum().item()\n",
        "        epoch_FP_TN += (predicted_labels!=batch.is_duplicate).sum().item()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_TP_FN/(epoch_TP_FN+epoch_FP_TN)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, thresh=0.5):\n",
        "    epoch_loss = 0\n",
        "    epoch_TP_FN = 0\n",
        "    epoch_FP_TN = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.question1, batch.question2)\n",
        "            loss = criterion(predictions, batch.is_duplicate)\n",
        "\n",
        "            predicted_labels = predictions >= thresh\n",
        "            epoch_TP_FN += (predicted_labels==batch.is_duplicate).sum().item()\n",
        "            epoch_FP_TN += (predicted_labels!=batch.is_duplicate).sum().item()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_TP_FN/(epoch_TP_FN+epoch_FP_TN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUgTV9ZWgwLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "# Track time taken\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} '\n",
        "          f'| Train Loss: {train_loss:.3f} '\n",
        "          f'| Train Accuracy: {train_acc:.3f} '\n",
        "          f'| Val. Loss: {valid_loss:.3f} '\n",
        "          f'| Val. Accuracy: {valid_acc:.3f} '\n",
        "          f'| Time taken: {time.time() - epoch_start_time:.2f}s'\n",
        "          f'| Time elapsed: {time.time() - start_time:.2f}s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpnufIu7gwN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in valid_iter:\n",
        "        predictions = model(batch.question1, batch.question2)\n",
        "        \n",
        "        predictions_list += predictions.tolist()\n",
        "        labels_list += batch.is_duplicate.tolist()\n",
        "        \n",
        "        # predictions_list += predictions\n",
        "\n",
        "predictions_list = np.array(predictions_list)\n",
        "labels_list = np.array(labels_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27QXdwYdbVfz",
        "colab_type": "code",
        "outputId": "a388af39-3e41-4890-f71a-9e6fc12ee69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "\n",
        "print(log_loss(labels_list, predictions_list))\n",
        "print(accuracy_score(labels_list, predictions_list>=0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9046713384677006\n",
            "0.8147122115313266\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}