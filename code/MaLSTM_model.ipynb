{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaLSTM_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP_uhltjglgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vincent notebook\n",
        "# Mount drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "%cd /content\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd '/content/gdrive/My Drive/NLP_Project/'\n",
        "%ls -l\n",
        "print(os.listdir())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kZs1qkrgv4i",
        "colab_type": "code",
        "outputId": "ff9c7789-6c7f-4db7-98d4-23124ba6091c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from string import punctuation\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import scipy.sparse as sp\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "# from gensim.test.utils import datapath, get_tmpfile\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data import Field, LabelField, TabularDataset, Iterator, BucketIterator\n",
        "from torchtext import vocab\n",
        "import torch.optim as optim\n",
        "\n",
        "import transform_dataset\n",
        "import compute_embeddings"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc1kTyPFqj2n",
        "colab_type": "text"
      },
      "source": [
        "### Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJhw75c3gv_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for google news, need to download and save in an appropriate format\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWROKB8E46qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "71a42566-abe9-4781-e657-f80dc631add3"
      },
      "source": [
        "wv.save_word2vec_format('embedding/word2vec-google-news-300')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAdr_6ln_qd",
        "colab_type": "code",
        "outputId": "2753339f-6727-40cd-b934-f97f528855e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from torchtext.vocab import Vectors\n",
        "vectors = Vectors(name='word2vec-google-news-300', cache='embedding')\n",
        "\n",
        "# Glove and FastText are easier to manage (download on the fly in the right format)\n",
        "# vectors = vocab.GloVe(name='600B', dim=300)\n",
        "# vectors = vocab.FastText(language='en')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3000000 [00:00<?, ?it/s]Skipping token b'3000000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 2999358/3000000 [18:28<00:00, 7616.65it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPtW6cWak6Rp",
        "colab_type": "code",
        "outputId": "f20e6e1e-62cd-43e1-e3ae-27d161be9bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# alternative with wiki simple (small embedding)\n",
        "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec -P ../embeddings/\n",
        "# TEXT.vocab.load_vectors('../embeddings/wiki.simple.vec')\n",
        "# vectors = Vectors('../embeddings/wiki.simple.vec')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-13 10:28:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 293187541 (280M) [binary/octet-stream]\n",
            "Saving to: ‘../embeddings/wiki.simple.vec’\n",
            "\n",
            "wiki.simple.vec     100%[===================>] 279.60M  11.9MB/s    in 25s     \n",
            "\n",
            "2020-05-13 10:29:25 (11.3 MB/s) - ‘../embeddings/wiki.simple.vec’ saved [293187541/293187541]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrO7qs1dqmeM",
        "colab_type": "text"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV8HvpY_kvAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"data/train.csv\"\n",
        "tokenizer = nltk.word_tokenize\n",
        "# tokenize = lambda x: x.split()\n",
        "# tokenizer = text_to_wordlist\n",
        "\n",
        "data = pd.read_csv(path)\n",
        "questions1 = data['question1'].astype('str').tolist()\n",
        "questions2 = data['question2'].astype('str').tolist()\n",
        "is_duplicates = data['is_duplicate'].tolist()\n",
        "\n",
        "TEXT = Field(\n",
        "        sequential=True,\n",
        "        tokenize = tokenizer,\n",
        "        pad_first = True,\n",
        "        dtype = torch.long,\n",
        "        fix_length = 20,\n",
        "        lower = True,\n",
        "        batch_first = True,\n",
        "        )\n",
        "TARGET = LabelField(use_vocab = False, dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YYTP3Uqmazw",
        "colab_type": "code",
        "outputId": "dd3004b0-0377-4394-9289-3b7dc1b2bc67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset = TabularDataset(path, 'csv', [('id', None),\n",
        "                                       ('qid1', None),\n",
        "                                       ('qid2', None),\n",
        "                                       ('question1', TEXT),\n",
        "                                       ('question2', TEXT),\n",
        "                                       ('is_duplicate', TARGET)],\n",
        "                         skip_header=True)\n",
        "\n",
        "train_data, valid_data = dataset.split(\n",
        "                  split_ratio=0.9,\n",
        "                  random_state=random.seed(42)\n",
        "                  )\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "\n",
        "# TEXT.build_vocab(train_data, vectors = vectors)\n",
        "TEXT.build_vocab(train_data)\n",
        "TARGET.build_vocab(train_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 363861\n",
            "Number of validation examples: 40429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNE3msM4e0Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An2aStAApVQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iter, valid_iter = BucketIterator.splits(\n",
        "        (train_data, valid_data), # we pass in the datasets we want the iterator to draw data from\n",
        "        batch_size=64,\n",
        "        device=device, # if you want to use the GPU, specify the GPU number here\n",
        "        sort_key=lambda x: len(x.question1)+len(x.question2), # the BucketIterator needs to be told what function it should use to group the data.\n",
        "        sort_within_batch=False\n",
        "        # repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX6lwMJFqs2S",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An3v4HD5gwC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class siameseNet(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, vectors = TEXT.vocab.vectors):\n",
        "    super().__init__()\n",
        "    # self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, mode='mean')\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "    self.pdist = nn.PairwiseDistance(p=1.0)\n",
        "\n",
        "  def forward_one(self, x):\n",
        "    output = self.embedding(x)\n",
        "    output = self.lstm(output)\n",
        "    return output\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "    emb1 = self.embedding(input1)\n",
        "    emb2 = self.embedding(input2)\n",
        "    output1, _ = self.lstm(emb1)\n",
        "    output2, _ = self.lstm(emb2)\n",
        "    output = torch.exp(-self.pdist(output1[:,-1,:], output2[:,-1,:]))\n",
        "    # output = nn.CosineSimilarity()(output1[:,-1,:], output2[:,-1,:])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdlGneh5gwFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_shape = TEXT.vocab.vectors.shape\n",
        "INPUT_DIM = emb_shape[0]\n",
        "EMBEDDING_DIM = emb_shape[1]\n",
        "HIDDEN_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = siameseNet(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = torch.nn.MSELoss()\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI7TJWQ3rIrD",
        "colab_type": "text"
      },
      "source": [
        "### Learning and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ1pYpkRgwIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(predictions, labels, thresh=0.5):\n",
        "    predicted_labels = predictions >= thresh\n",
        "    accuracy = (predicted_labels == labels).sum()/len(predictions)\n",
        "    return accuracy.item()\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, thresh=0.5):\n",
        "    # Track the loss\n",
        "    epoch_loss = 0\n",
        "    epoch_TP_FN = 0\n",
        "    epoch_FP_TN = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.question1, batch.question2)\n",
        "        loss = criterion(predictions, batch.is_duplicate)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted_labels = predictions >= thresh\n",
        "        epoch_TP_FN += (predicted_labels==batch.is_duplicate).sum().item()\n",
        "        epoch_FP_TN += (predicted_labels!=batch.is_duplicate).sum().item()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_TP_FN/(epoch_TP_FN+epoch_FP_TN)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, thresh=0.5):\n",
        "    epoch_loss = 0\n",
        "    epoch_TP_FN = 0\n",
        "    epoch_FP_TN = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.question1, batch.question2)\n",
        "            loss = criterion(predictions, batch.is_duplicate)\n",
        "\n",
        "            predicted_labels = predictions >= thresh\n",
        "            epoch_TP_FN += (predicted_labels==batch.is_duplicate).sum().item()\n",
        "            epoch_FP_TN += (predicted_labels!=batch.is_duplicate).sum().item()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_TP_FN/(epoch_TP_FN+epoch_FP_TN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUgTV9ZWgwLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "7232c5fc-fd51-4cbc-efba-29949aff1655"
      },
      "source": [
        "N_EPOCHS = 11\n",
        "\n",
        "# Track time taken\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} '\n",
        "          f'| Train Loss: {train_loss:.3f} '\n",
        "          f'| Train Accuracy: {train_acc:.3f} '\n",
        "          f'| Val. Loss: {valid_loss:.3f} '\n",
        "          f'| Val. Accuracy: {valid_acc:.3f} '\n",
        "          f'| Time taken: {time.time() - epoch_start_time:.2f}s'\n",
        "          f'| Time elapsed: {time.time() - start_time:.2f}s')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.176 | Train Accuracy: 0.757 | Val. Loss: 0.158 | Val. Accuracy: 0.787 | Time taken: 95.27s| Time elapsed: 95.27s\n",
            "| Epoch: 02 | Train Loss: 0.143 | Train Accuracy: 0.811 | Val. Loss: 0.148 | Val. Accuracy: 0.801 | Time taken: 98.24s| Time elapsed: 193.51s\n",
            "| Epoch: 03 | Train Loss: 0.126 | Train Accuracy: 0.838 | Val. Loss: 0.145 | Val. Accuracy: 0.805 | Time taken: 106.98s| Time elapsed: 300.52s\n",
            "| Epoch: 04 | Train Loss: 0.114 | Train Accuracy: 0.857 | Val. Loss: 0.142 | Val. Accuracy: 0.811 | Time taken: 105.72s| Time elapsed: 406.24s\n",
            "| Epoch: 05 | Train Loss: 0.105 | Train Accuracy: 0.872 | Val. Loss: 0.140 | Val. Accuracy: 0.815 | Time taken: 105.64s| Time elapsed: 511.89s\n",
            "| Epoch: 06 | Train Loss: 0.097 | Train Accuracy: 0.884 | Val. Loss: 0.142 | Val. Accuracy: 0.811 | Time taken: 106.04s| Time elapsed: 617.93s\n",
            "| Epoch: 07 | Train Loss: 0.091 | Train Accuracy: 0.893 | Val. Loss: 0.140 | Val. Accuracy: 0.815 | Time taken: 93.25s| Time elapsed: 711.18s\n",
            "| Epoch: 08 | Train Loss: 0.086 | Train Accuracy: 0.900 | Val. Loss: 0.141 | Val. Accuracy: 0.815 | Time taken: 98.18s| Time elapsed: 809.37s\n",
            "| Epoch: 09 | Train Loss: 0.082 | Train Accuracy: 0.906 | Val. Loss: 0.139 | Val. Accuracy: 0.817 | Time taken: 106.25s| Time elapsed: 915.62s\n",
            "| Epoch: 10 | Train Loss: 0.078 | Train Accuracy: 0.912 | Val. Loss: 0.141 | Val. Accuracy: 0.816 | Time taken: 106.82s| Time elapsed: 1022.44s\n",
            "| Epoch: 11 | Train Loss: 0.075 | Train Accuracy: 0.917 | Val. Loss: 0.142 | Val. Accuracy: 0.814 | Time taken: 106.44s| Time elapsed: 1128.88s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpnufIu7gwN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in valid_iter:\n",
        "        predictions = model(batch.question1, batch.question2)\n",
        "        \n",
        "        predictions_list += predictions.tolist()\n",
        "        labels_list += batch.is_duplicate.tolist()\n",
        "        \n",
        "        # predictions_list += predictions\n",
        "\n",
        "predictions_list = np.array(predictions_list)\n",
        "labels_list = np.array(labels_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27QXdwYdbVfz",
        "colab_type": "code",
        "outputId": "34efdbeb-3b73-4578-c77c-da218d5a8434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "\n",
        "print(log_loss(labels_list, predictions_list))\n",
        "print(accuracy_score(labels_list, predictions_list>=0.5))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1362893153164952\n",
            "0.8143411907294269\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}